{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84108d9a",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd86d8a",
   "metadata": {},
   "source": [
    "## Numerical Solution - Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee4aa3",
   "metadata": {},
   "source": [
    "### Practical Part\n",
    "- Implement gradient descent for some simple functions.\n",
    "- See the functions in the notebook.\n",
    "- Vary the learning rate. \n",
    "- Consider the simultaneous optimization problem. What is the implication of your findings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3e253f",
   "metadata": {},
   "source": [
    "### Understanding the learning rate\n",
    "Consider the following functions:\n",
    "- $f_1(x) = x^2$\n",
    "- $f_2(x) = x^3$\n",
    "- $f_3(x) = \\sin (x) + 0.01\\cdot (x- 1.5\\pi )^2$\n",
    "\n",
    "Find their absolute minima by gradient descent. Choose different starting values $x_0$ and vary the learning rate from very small to very large values. \n",
    "Plot the function and plot each step of gradient descent i.e. the series $x_0$, $x_1$, ... until the minimum $x_\\text{min}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8889986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def f1(x):\n",
    "    return x**2\n",
    "\n",
    "def f1_prime(x):\n",
    "    return 2*x\n",
    "\n",
    "def f2(x):\n",
    "    return x**3\n",
    "\n",
    "def f2_prime(x):\n",
    "    return 3*x**2\n",
    "\n",
    "def f3(x):\n",
    "    return np.sin(x) + 0.01*(x- 1.5*math.pi)**2\n",
    "\n",
    "def f3_prime(x):\n",
    "    return np.cos(x) + 2.*0.01*(x-1.5*math.pi)\n",
    "\n",
    "\n",
    "# TODO\n",
    "def plot_gd(f, f_prime, x0, lr, epochs):\n",
    "    \"\"\"This function realizes gradient descent algorithm on the function f. It creates a plot of the function \n",
    "    inccuding all gradient descent steps. \n",
    "    Inputs:\n",
    "     - f: function\n",
    "     - f_prime: first derivate function of f\n",
    "     - x0: initial value to start gradient descent from\n",
    "     - lr: learning rate\n",
    "     - epochs: number of iterations\n",
    "     \n",
    "    \"\"\"\n",
    "    # left and right limits of the plot\n",
    "    lo = min(x0, -x0) \n",
    "    hi = max(x0, -x0)\n",
    "    # x-values of the plot, step size 0.01 * plotting range \n",
    "    xvals = np.arange(lo, hi, 0.01*(hi-lo))\n",
    "    plt.plot(xvals, f(xvals))\n",
    "    x = x0\n",
    "    for i in range (epochs):\n",
    "        plt.plot(x, f(x), 'r*')\n",
    "\n",
    "        # TODO insert the gradient descent step\n",
    "\n",
    "\n",
    "    print ()\n",
    "    print (\"Final value =\", x)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c07b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = # TODO - try different values (remember, search a suitable learning rate over orders of magnitude!)\n",
    "EPOCHS = 100\n",
    "x = -3 # initial value - change, if needed\n",
    "\n",
    "plot_gd(f1, f1_prime, x, lr, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af03505",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = # TODO\n",
    "EPOCHS = 100\n",
    "x = -3 # initial value\n",
    "\n",
    "plot_gd(f1, f1_prime, x, lr, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b8f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = # TODO\n",
    "EPOCHS = 100\n",
    "x = 3 # initial value\n",
    "\n",
    "plot_gd(f2, f2_prime, x, lr, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40358663",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = # TODO\n",
    "EPOCHS = 1000\n",
    "x = # TODO try various initial values\n",
    "\n",
    "plot_gd(f3, f3_prime, x, lr, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a0060c",
   "metadata": {},
   "source": [
    "### Implement GD for linear regression\n",
    "Implement gradient descent for the following toy problem:\n",
    "You have two features $x_1$ and $x_2$. Your target $y=0.5x_1 + 0.3x_2 + \\epsilon$ with $\\epsilon\\sim \\mathcal{N}(0; \\sigma^2)$.\n",
    "\n",
    "Solve the linear regression with __gradient descent__.\n",
    "\n",
    "\n",
    "Find a good learning rate $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ac356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "822a0764",
   "metadata": {},
   "source": [
    "### Simultaneous optimization\n",
    "\n",
    "Optimize $f: \\mathbb{R}^2\\to\\mathbb{R}^2$, $f(x_1, x_2) = (x_1^2, (a\\cdot x_2)^2)$.  \n",
    "Look at different values for $a$, e.g. $a=2$, $a=5$, $a=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1495a302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa3caef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76c215e9",
   "metadata": {},
   "source": [
    "## Linear regression examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d5c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc6c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_diabetes(return_X_y=False, as_frame=True, scaled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a0dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffbc59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data.values\n",
    "y = data.target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e328292",
   "metadata": {},
   "source": [
    "### Practical Part 1 - Implement Linear Regression \n",
    "- Split the data randomly into train and test sets (75\\%/25\\% is fine).\n",
    "- Predict the target variable with linear regression.\n",
    "- Use the scikit-learn implementation.\n",
    "- Measure the errors of your prediction and try to figure out where the errors occur. \n",
    "- Ablation study: Study the effect of data normalization on the quality of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9884286a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2476e8c8",
   "metadata": {},
   "source": [
    "### Practical Part 2 - Regularization\n",
    "- Until now, only the features themselves have been considered. \n",
    "- Add non-linear terms representing the interactions between the features.\n",
    "- For second order, these terms are $x_ix_j$ for features $x_i$ and $x_j$, third order $x_ix_jx_k$, and so on.\n",
    "- Limit yourself to third order and interaction terms only ($i,j,k$ mutually different)\n",
    "- Re-do linear regression with these additional features.\n",
    "- What do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bfd1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28213f28",
   "metadata": {},
   "source": [
    "### Practical Part 3 - Ridge Regression and Lasso\n",
    "- Use Ridge and Lasso for the extended diabetes dataset (including the interaction terms)\n",
    "- Look into the scikit-learn documentation to learn about the parameter ''alpha''. \n",
    "- For which values of alpha do you get the best model?\n",
    "- When applying Lasso, figure out which features have a non-zero coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d2af7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malenv",
   "language": "python",
   "name": "malenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
